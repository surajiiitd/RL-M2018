{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed \n",
    "# Reference : https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/master/chapter05/blackjack.py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_player_initialisation(ACTION_HIT,ACTION_STAND):\n",
    "    '''Initialising the policy for player\n",
    "    '''\n",
    "    POLICY_PLAYER = np.zeros(22, dtype=np.int)\n",
    "    for i in range(12, 20):\n",
    "            POLICY_PLAYER[i] = ACTION_HIT\n",
    "    POLICY_PLAYER[20] = ACTION_STAND\n",
    "    POLICY_PLAYER[21] = ACTION_STAND\n",
    "    return POLICY_PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_blackjack():\n",
    "    '''Initializing the parameters and vairables for blackjack\n",
    "    '''\n",
    "    # actions: hit or stand\n",
    "    ACTION_HIT = 0\n",
    "    ACTION_STAND = 1  #  \"strike\" in the book\n",
    "    ACTIONS = [ACTION_HIT, ACTION_STAND]\n",
    "    # policy for player\n",
    "    POLICY_PLAYER  = policy_player_initialisation(ACTION_HIT,ACTION_STAND)\n",
    "    return ACTION_HIT,ACTION_STAND,ACTIONS,POLICY_PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_title(action_usable_ace, state_value_usable_ace,action_no_usable_ace, state_value_no_usable_ace):\n",
    "    '''Function to give the value to the plots \n",
    "    \n",
    "    '''\n",
    "    images = []\n",
    "    \n",
    "    k = [action_usable_ace,\n",
    "          state_value_usable_ace,\n",
    "          action_no_usable_ace,\n",
    "          state_value_no_usable_ace]\n",
    "    l = ['Optimal policy with usable Ace',\n",
    "              'Optimal value with usable Ace',\n",
    "              'Optimal policy without usable Ace',\n",
    "              'Optimal value without usable Ace']\n",
    "    for i in k:\n",
    "        images.append(i)\n",
    "    for i in l:\n",
    "        titles.append(i)\n",
    "        \n",
    "    return images,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_policy_player(usable_ace_player, player_sum, dealer_card):\n",
    "    '''Function form of target policy of player'''\n",
    "    \n",
    "    return POLICY_PLAYER[player_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_policy_player(usable_ace_player, player_sum, dealer_card):\n",
    "    '''Function form of behavior policy of player'''\n",
    "    \n",
    "    if np.random.binomial(1, 0.5) != 1:\n",
    "        return ACTION_HIT\n",
    "    return ACTION_STAND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_dealer_initialisation(ACTION_HIT,ACTION_STAND):\n",
    "    '''Initialisation for policy of dealer \n",
    "    '''\n",
    "    # policy for dealer\n",
    "    POLICY_DEALER = np.zeros(22, dtype=np.int)\n",
    "    min = 12\n",
    "    max = 17\n",
    "    max_l = 22\n",
    "    for i in range(min, max):\n",
    "        POLICY_DEALER[i] = ACTION_HIT\n",
    "    for i in range(max, max_l):\n",
    "        POLICY_DEALER[i] = ACTION_STAND\n",
    "    return POLICY_DEALER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_card(card):\n",
    "    card = min(card, 10)\n",
    "    return card\n",
    "    \n",
    "def get_card():\n",
    "    ''' get a new card\n",
    "    ''' \n",
    "    card = np.random.randint(1, 14)\n",
    "    card = get_min(card)\n",
    "    \n",
    "    return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the value of a card (11 for ace).\n",
    "def card_value(card_id):\n",
    "    '''Function to return the card value\n",
    "    Parameters are as: \n",
    "    card_id : Card id \n",
    "    \n",
    "    '''\n",
    "    if card_id == 1:\n",
    "        return 11\n",
    "\n",
    "    else:\n",
    "        return card_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_initial_state(initial_state,initial_action,policy_player,player_sum,\n",
    "                                                                                      player_trajectory,usable_ace_player,dealer_card1,dealer_card2,usable_ace_dealer):\n",
    "    '''Condition when initial_state is None or Not None\n",
    "    '''\n",
    "    if initial_state is not None:\n",
    "            # generate a random initial state\n",
    "            while player_sum < 12:\n",
    "                # if sum of player is less than 12, always hit\n",
    "                card = get_card()\n",
    "                player_sum += card_value(card)\n",
    "                # If the player's sum is larger than 21, he may hold one or two aces.\n",
    "                if player_sum <= 21:\n",
    "                    usable_ace_player |= (1 == card)\n",
    "                    \n",
    "                else:\n",
    "                    assert player_sum == 22\n",
    "                    # last card must be ace\n",
    "                    player_sum -= 10\n",
    "            # initialize cards of dealer, suppose dealer will show the first card he gets\n",
    "            dealer_card1 = get_card()\n",
    "            dealer_card2 = get_card()\n",
    "    else:\n",
    "        # use specified initial state\n",
    "        usable_ace_player, player_sum, dealer_card1 = initial_state\n",
    "        ###\n",
    "        ##\n",
    "        dealer_card2 = get_card()\n",
    "    return dealer_card1,dealer_card2,usable_ace_player, player_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sum(player_sum,dealer_sum,state,player_trajectory):\n",
    "    '''compare the sum between player and dealer\n",
    "    Parameters are as : \n",
    "    player_sum : player sum\n",
    "    dealer_sum : dealer sum\n",
    "    state_player : state of palyer\n",
    "    player_trajectory : Trajectory of player\n",
    "    \n",
    "    '''\n",
    "    assert player_sum <= 21 and dealer_sum <= 21\n",
    "    \n",
    "    if player_sum > dealer_sum:\n",
    "        return state, 1, player_trajectory\n",
    "    elif player_sum == dealer_sum:\n",
    "        return state, 0, player_trajectory\n",
    "    else:\n",
    "        return state, -1, player_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_play(policy_player, initial_state=None, initial_action=None):\n",
    "     # player status\n",
    "    # trajectory of player\n",
    "    player_trajectory = []\n",
    "    # whether player uses Ace as 1\n",
    "    # dealer status\n",
    "    dealer_card1 = dealer_card2 = player_sum = 0\n",
    "    usable_ace_dealer = usable_ace_player = False\n",
    "    dealer_card1,dealer_card2,usable_ace_player, player_sum = condition_initial_state(initial_state,initial_action,policy_player,player_sum,\n",
    "                                                                                      player_trajectory,usable_ace_player,dealer_card1,dealer_card2,usable_ace_dealer)\n",
    "\n",
    "    # initial state of the game\n",
    "    state = [usable_ace_player, player_sum, dealer_card1]\n",
    "\n",
    "    # initialize dealer's sum\n",
    "    dealer_sum = card_value(dealer_card1) + card_value(dealer_card2)\n",
    "    usable_ace_dealer = 1 in (dealer_card1, dealer_card2)\n",
    "    # if the dealer's sum is larger than 21, he must hold two aces.\n",
    "    if dealer_sum > 21:\n",
    "        assert dealer_sum == 22\n",
    "        # use one Ace as 1 rather than 11\n",
    "        dealer_sum -= 10\n",
    "    assert dealer_sum <= 21\n",
    "    assert player_sum <= 21\n",
    "    return dealer_card1,dealer_card2,usable_ace_player, player_sum,usable_ace_dealer,player_trajectory,state,dealer_sum\n",
    "    \n",
    "def play(policy_player, initial_state=None, initial_action=None):\n",
    "    '''play a game\n",
    "        policy_player: specify policy for player\n",
    "        initial_state: [whether player has a usable Ace, sum of player's cards, one card of dealer]\n",
    "        initial_action: the initial action\n",
    "        Parameters are as :\n",
    "        policy_player : policy of player\n",
    "        initial_state  : initial state \n",
    "        initial_action :  initial action\n",
    "        \n",
    "        '''\n",
    "    dealer_card1,dealer_card2,usable_ace_player, player_sum,usable_ace_dealer,player_trajectory,state,dealer_sum = initial_play(policy_player, \n",
    "                                                                                                                                initial_state=None, initial_action=None)\n",
    "\n",
    "    # player's turn\n",
    "    while True:\n",
    "        if initial_action is not None:\n",
    "            action = initial_action\n",
    "            initial_action = None\n",
    "        else:\n",
    "            # get action based on current sum\n",
    "            action = policy_player(usable_ace_player, player_sum, dealer_card1)\n",
    "\n",
    "        # track player's trajectory for importance sampling\n",
    "        player_trajectory.append([(usable_ace_player, player_sum, dealer_card1), action])\n",
    "        if action == ACTION_STAND:\n",
    "            break\n",
    "        # if hit, get new card\n",
    "        card = get_card()\n",
    "        # Keep track of the ace count. the usable_ace_player flag is insufficient alone as it cannot\n",
    "        # distinguish between having one ace or two.\n",
    "        ace_count = int(usable_ace_player)\n",
    "        if card == 1:\n",
    "            ace_count += 1\n",
    "        player_sum += card_value(card)\n",
    "        # If the player has a usable ace, use it as 1 to avoid busting and continue.\n",
    "        while player_sum > 21 and ace_count:\n",
    "            player_sum -= 10\n",
    "            ace_count -= 1\n",
    "        # player busts\n",
    "        if player_sum > 21:\n",
    "            return state, -1, player_trajectory\n",
    "        assert player_sum <= 21\n",
    "        usable_ace_player = (ace_count == 1)\n",
    "\n",
    "    # dealer's turn\n",
    "    while True:\n",
    "        # get action based on current sum\n",
    "#         print(POLICY_DEALER)\n",
    "        action = POLICY_DEALER[dealer_sum]\n",
    "        \n",
    "        if action == ACTION_STAND:\n",
    "            break\n",
    "        # if hit, get a new card\n",
    "        new_card = get_card()\n",
    "        ace_count = int(usable_ace_dealer)\n",
    "        if new_card == 1:\n",
    "            ace_count += 1\n",
    "        dealer_sum += card_value(new_card)\n",
    "        # If the dealer has a usable ace, use it as 1 to avoid busting and continue.\n",
    "        while dealer_sum > 21 and ace_count:\n",
    "            dealer_sum -= 10\n",
    "            ace_count -= 1\n",
    "        # dealer busts\n",
    "        if dealer_sum > 21:\n",
    "            return state, 1, player_trajectory\n",
    "        usable_ace_dealer = (ace_count == 1)\n",
    "    state,reward,player_trajectory = compare_sum(player_sum,dealer_sum,state,player_trajectory)\n",
    "    return state,reward,player_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Sample with Off-Policy\n",
    "def monte_off_loop(episodes,behavior_policy_player,initial_state,rhos,returns):\n",
    "    for i in range(0, episodes):\n",
    "        _, reward, player_trajectory = play(behavior_policy_player, initial_state=initial_state)\n",
    "\n",
    "        # get the importance ratio\n",
    "        numerator = denominator =  1.0\n",
    "        for (usable_ace, player_sum, dealer_card), action in player_trajectory:\n",
    "            if action != target_policy_player(usable_ace, player_sum, dealer_card):\n",
    "                numerator = 0.0\n",
    "                break\n",
    "            else:\n",
    "                denominator *= 0.5\n",
    "                \n",
    "        rho = numerator / denominator\n",
    "        rhos.append(rho)\n",
    "        returns.append(reward)\n",
    "    return rhos,returns\n",
    "\n",
    "def monte_carlo_off_policy(episodes):\n",
    "    initial_state = [True, 13, 2]\n",
    "\n",
    "    rhos = []\n",
    "    returns = []\n",
    "    rhos,returns = monte_off_loop(episodes,behavior_policy_player,initial_state,rhos,returns)\n",
    "    rhos = np.asarray(rhos)\n",
    "    rhos = rhos.tolist()\n",
    "    rhos = np.array(rhos)\n",
    "    returns = np.asarray(returns)\n",
    "    returns = returns.tolist()\n",
    "    returns = np.array(returns)\n",
    "    w_r = rhos * returns\n",
    "    ####\n",
    "    weighted_returns = copy.deepcopy(w_r) \n",
    "    weighted_returns = np.add.accumulate(weighted_returns)\n",
    "    rhos = np.add.accumulate(rhos)\n",
    "    arange_np = np.arange(1, episodes + 1)\n",
    "    ordinary_sampling = weighted_returns / arange_np\n",
    "    with np.errstate(divide='ignore',invalid='ignore'):\n",
    "        check_w = weighted_returns / rhos\n",
    "        w_s = np.where(rhos != 0,check_w, 0)\n",
    "        weighted_sampling = copy.deepcopy(w_s)\n",
    "    return ordinary_sampling, weighted_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fig(error_ordinary,error_weighted,num):\n",
    "    plt.plot(error_ordinary, label='Ordinary Importance Sampling')\n",
    "    plt.plot(error_weighted, label='Weighted Importance Sampling')\n",
    "    plt.xlabel('Episodes (log scale)')\n",
    "    plt.ylabel('Mean square error')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('../images1/figure_5_'+str(num)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting the initial parameters \n",
    "ACTION_HIT,ACTION_STAND,ACTIONS,POLICY_PLAYER = initialisation_blackjack()\n",
    "POLICY_DEALER = policy_dealer_initialisation(ACTION_HIT,ACTION_STAND)\n",
    "\n",
    "true_value = -0.27726\n",
    "episodes = 10000\n",
    "runs = 100\n",
    "error_ordinary = np.zeros(episodes)\n",
    "error_weighted = np.zeros(episodes)\n",
    "for i in tqdm(range(0, runs)):\n",
    "    ordinary_sampling_, weighted_sampling_ = monte_carlo_off_policy(episodes)\n",
    "    # get the squared error\n",
    "    power_val = np.power(ordinary_sampling_ - true_value, 2)\n",
    "    error_ordinary =error_ordinary +power_val\n",
    "    power_val = np.power(weighted_sampling_ - true_value, 2)\n",
    "    error_weighted =error_weighted + power_val\n",
    "error_ordinary =error_ordinary/runs\n",
    "error_weighted = error_weighted/runs\n",
    "\n",
    "draw_fig(error_ordinary,error_weighted,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
